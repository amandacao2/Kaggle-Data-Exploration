{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4425c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a0994",
   "metadata": {},
   "source": [
    "stuff I want to try differently this time:\n",
    "\n",
    "-implementing a pipeline\n",
    "\n",
    "-removing correlated features\n",
    "\n",
    "-use logistic scaling on target value\n",
    "\n",
    "-use boxcox scaling on skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6ac72734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.drop(\"Id\", inplace=True, axis=1)\n",
    "df.head()\n",
    "\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8debbc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES THAT ARE HIGHLY CORRELATED TO EACH OTHER SUMMARY:\n",
      " ============================================================\n",
      "('GarageCars', 'GarageArea') have a correlation of: 0.882475414281462\n",
      "('GarageArea', 'GarageCars') have a correlation of: 0.882475414281462\n",
      "('YearBuilt', 'GarageYrBlt') have a correlation of: 0.8256674841743408\n",
      "('GarageYrBlt', 'YearBuilt') have a correlation of: 0.8256674841743408\n",
      "('GrLivArea', 'TotRmsAbvGrd') have a correlation of: 0.8254893743088425\n",
      "('TotRmsAbvGrd', 'GrLivArea') have a correlation of: 0.8254893743088425\n",
      "('TotalBsmtSF', '1stFlrSF') have a correlation of: 0.8195299750050339\n",
      "('1stFlrSF', 'TotalBsmtSF') have a correlation of: 0.8195299750050339\n",
      "('SalePrice', 'OverallQual') have a correlation of: 0.7909816005838053\n",
      "('OverallQual', 'SalePrice') have a correlation of: 0.7909816005838053\n",
      "('GrLivArea', 'SalePrice') have a correlation of: 0.7086244776126515\n",
      "('SalePrice', 'GrLivArea') have a correlation of: 0.7086244776126515\n",
      "('2ndFlrSF', 'GrLivArea') have a correlation of: 0.6875010641666033\n",
      "('GrLivArea', '2ndFlrSF') have a correlation of: 0.6875010641666033\n",
      "('TotRmsAbvGrd', 'BedroomAbvGr') have a correlation of: 0.676619935742649\n",
      "('BedroomAbvGr', 'TotRmsAbvGrd') have a correlation of: 0.676619935742649\n"
     ]
    }
   ],
   "source": [
    "#removing correlated features \n",
    "corr = df[num_cols].corr()\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# sns.heatmap(corr, annot=False, cmap='coolwarm', center=0)\n",
    "# plt.title('Feature Correlation Heatmap')\n",
    "# plt.show()\n",
    "\n",
    "corrs = corr.abs().unstack().sort_values(ascending=False)\n",
    "corrs = corrs[corrs != 1]\n",
    "\n",
    "print(\"FEATURES THAT ARE HIGHLY CORRELATED TO EACH OTHER SUMMARY:\\n\", \"=\"*60)\n",
    "i = 0\n",
    "for x in corrs:\n",
    "    print(f\"{corrs.index[i]} have a correlation of: {x}\")\n",
    "    i += 1\n",
    "    if i == 16:  \n",
    "        break\n",
    "\n",
    "# Removing highly correlated features\n",
    "drop_cols = ['GarageCars','GarageYrBlt',]\n",
    "df.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cda87438",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"SalePrice\", axis=1)\n",
    "y = np.log1p(df[\"SalePrice\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5dae5dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.08425502219844394\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('yeojohnson', PowerTransformer(method='yeo-johnson')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "bundle = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "bundle.fit(X_train, y_train)\n",
    "\n",
    "predictions = bundle.predict(X_test)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a2399",
   "metadata": {},
   "source": [
    "First MAE: 19488.45336180804\n",
    "\n",
    "Second MAE: MAE: 0.08455341302003157 (after log scaling the sales price WTF...)\n",
    "\n",
    "third MAE: MAE: 0.11195828082943877 (after dropping 5 correlated features)\n",
    "\n",
    "fourth MAE: MAE: 0.08428716889073772 (after dropping 3 correlated features)\n",
    "\n",
    "***fith MAE: MAE: 0.08425502219844394 (after dropping 2 correlated features)\n",
    "\n",
    "sixth MAE: MAE: 0.08474545986700253 (after dropping 1 correlated features)\n",
    "\n",
    "seventh MAE: MAE: 0.08931800056467047 (removing yeo johnston from pipeline, 2 corr features)\n",
    "\n",
    "eigth MAE: MAE: 0.08430445415454502 (after dropping standard scalar, 2 corr features)\n",
    "\n",
    "ninth MAE: MAE: 0.09735830740464979 (after dropping both, 2 corr features)\n",
    "\n",
    "tenth MAE: MAE: 0.11378258438385334 (after changing imputing method to median)\n",
    "\n",
    "eleventh MAE: MAE: 0.08436959715402965 (after chaning imputing method to mean)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a0ddcf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ATTEMPT 2 SUMMARY:\n",
      " ===========================================================================\n",
      "Original MAE is 142.45% of first attempt MAE.\n",
      "Original MAE is 32948537.70% of best attempt MAE.\n",
      "From 1st attempt to best attempt, the MAE improved by 231303.17%\n",
      "\n",
      "\n",
      "yay!\n",
      " ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING ATTEMPT 2 SUMMARY:\\n\", \"=\"*75)\n",
    "original_mae = 27760.797752343573\n",
    "first_attempt_mae = 19488.45336180804\n",
    "best_mae = 0.08425502219844394\n",
    "\n",
    "improvement_first = ((original_mae - first_attempt_mae) / original_mae) * 100\n",
    "improvement_best = ((original_mae - best_mae) / original_mae) * 100\n",
    "\n",
    "factor_first = original_mae / first_attempt_mae\n",
    "factor_best = original_mae / best_mae\n",
    "\n",
    "percent_first = (original_mae / first_attempt_mae) * 100\n",
    "percent_best = (original_mae / best_mae) * 100\n",
    "perecent = (first_attempt_mae / best_mae)\n",
    "print(f\"Original MAE is {percent_first:.2f}% of first attempt MAE.\")\n",
    "print(f\"Original MAE is {percent_best:.2f}% of best attempt MAE.\")\n",
    "print(f\"From 1st attempt to best attempt, the MAE improved by {perecent:.2f}%\")\n",
    "print(\"\\n\\nyay!\\n\", \"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6270dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: note some observations from above..\n",
    "# main Q-> why was logistic scaling so impactful and understand it and understand code esp pipeline\n",
    "# next step trying different models and incorperate hyperparameter tuning,\n",
    "# features selection and cross-validation\n",
    "# look into finding a way to automate this process for the future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
